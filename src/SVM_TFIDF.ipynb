{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocessing import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier with TF-IDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############For Training Data#####################\n",
    "# Read the data from the csv file named 'preprocessedData.csv' arabic data\n",
    "train = pd.read_csv('../Dataset/cleaned_train.csv', encoding='utf-8')\n",
    "# Unpack the data into text and stance\n",
    "Train_X = train['text']\n",
    "stance_Train_Y = train['stance']\n",
    "cat_Train_Y = train['category']\n",
    "\n",
    "##############For Testing Data#####################\n",
    "test = pd.read_csv('../Dataset/cleaned_dev.csv', encoding='utf-8')\n",
    "# Perform the data preprocessing\n",
    "test = clean_data(test)\n",
    "# Unpack the data into text, and stance\n",
    "Test_X = test['text']\n",
    "stance_Test_Y = test['stance']\n",
    "cat_Test_Y = test['category']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we want to calculate the TF-IDF score for each word in the corpus\n",
    "\n",
    "Tfidf_vect = TfidfVectorizer(max_features=27000)\n",
    "Tfidf_vect.fit(train['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6988, 26884)\n"
     ]
    }
   ],
   "source": [
    "print(Train_X_Tfidf.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SMOTE to the training data to balance the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.0\n",
      "Counter({1: 5538, 0: 1012, -1: 438})\n",
      "Counter({1: 5538, 0: 5538, -1: 5538})\n"
     ]
    }
   ],
   "source": [
    "# Here we want to apply SMOTE to the data to balance the data against 3 classes\n",
    "# check version number\n",
    "import imblearn\n",
    "print(imblearn.__version__)\n",
    "# Count the number of each class\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "print(Counter(Train_Y))\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "SMOTE_Train_X_Tfidf, SMOTE_Train_Y = oversample.fit_resample(Train_X_Tfidf, Train_Y)\n",
    "print(Counter(SMOTE_Train_Y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model agains the unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  81.8\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "# What is the best kernel for SVM in the case of multi-class classification? - Quora\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for each class ->  [0.24444444 0.25301205 0.90137615]\n",
      "Macro Average F1 score ->  0.46627754647540215\n"
     ]
    }
   ],
   "source": [
    "# Calculate the F1 score for each class\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score for each class -> \",f1_score(Test_Y, predictions_SVM, average=None))\n",
    "# Calculate the Macro Average F1 score for the whole data\n",
    "print(\"Macro Average F1 score -> \",f1_score(Test_Y, predictions_SVM, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model against the balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  80.0\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "# What is the best kernel for SVM in the case of multi-class classification? - Quora\n",
    "SMOTE_SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SMOTE_SVM.fit(SMOTE_Train_X_Tfidf,SMOTE_Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SMOTE_SVM = SMOTE_SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SMOTE_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for each class ->  [0.34710744 0.41463415 0.89161053]\n",
      "F1 score for the whole data ->  0.5511173723732602\n"
     ]
    }
   ],
   "source": [
    "# Calculate the F1 score for each class\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score for each class -> \",f1_score(Test_Y, predictions_SMOTE_SVM, average=None))\n",
    "# Calculate the F1 score for the whole data\n",
    "print(\"F1 score for the whole data -> \",f1_score(Test_Y, predictions_SMOTE_SVM, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
