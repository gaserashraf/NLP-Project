{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the csv file named 'preprocessedData.csv' arabic data\n",
    "data = pd.read_csv('./Dataset/preprocessingData.csv', encoding='utf-8')\n",
    "# Remove [ ] from each word in the data\n",
    "# data['text'] = data['text'].str.replace('[^\\w\\s]', '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farasa.stemmer import FarasaStemmer\n",
    "stemmer = FarasaStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hazem\\AppData\\Local\\Temp\\ipykernel_9160\\2722046469.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'][i]=newTweet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(data['text'])):\n",
    "    # remove the Links\n",
    "    newTweet=re.sub(r'http\\S+', '', data['text'][i])\n",
    "    # remove english letters\n",
    "    newTweet=re.sub(r'[a-zA-Z]', '', newTweet)\n",
    "    # remove numbers\n",
    "    newTweet=re.sub(r'[0-9]', '', newTweet)\n",
    "    # remove the arabic numbers\n",
    "    newTweet=re.sub(r'[\\u0660-\\u0669]', '', newTweet)\n",
    "    # remove the emails\n",
    "    newTweet=re.sub(r'\\S*@\\S*\\s?', '', newTweet)\n",
    "    # remove the hashtags\n",
    "    newTweet=re.sub(r'#\\S+', '', newTweet)\n",
    "    # remove the mentions\n",
    "    newTweet=re.sub(r'@\\S+', '', newTweet)\n",
    "    # remove emojis\n",
    "    RE_EMOJI = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    newTweet=RE_EMOJI.sub(r'', newTweet)\n",
    "    # replace _ by whitespace\n",
    "    newTweet=re.sub(r'_', ' ', newTweet)\n",
    "    \n",
    "    # remove the punctuations\n",
    "    newTweet = re.sub('\\W+',' ', newTweet)\n",
    "\n",
    "    # remove duplicated whitespaces\n",
    "    newTweet=re.sub(r'\\s+', ' ', newTweet)\n",
    "\n",
    "    # applay tokenization\n",
    "    newTweet=newTweet.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    stopwords_arabic = nltk.corpus.stopwords.words('arabic')\n",
    "    stopwords_arabic.append('ال')\n",
    "    newTweet=[word for word in newTweet if word not in stopwords_arabic]\n",
    "\n",
    "    # join the words\n",
    "    newTweet=' '.join(newTweet)\n",
    "\n",
    "    # apply stemming\n",
    "    # newTweet=stemmer.stem(newTweet)\n",
    "    \n",
    "    # add the new tweet to the list\n",
    "    data['text'][i]=newTweet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the data into text, category and stance\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(data['text'],data['stance'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بيل غيتس يتلقى لقاح تصوير الابرة السيرنجة الدواء لابس بولو صيفي عز الشتاء يقول ان مزايا عمر عام انه مؤهل للحصول اللقاح يعنى يحتاج اللقاح عمره اصغر\n"
     ]
    }
   ],
   "source": [
    "# Here we want to calculate the TF-IDF score for each word in the corpus\n",
    "Tfidf_vect = TfidfVectorizer(max_features=27000)\n",
    "\n",
    "\n",
    "Tfidf_vect.fit(data['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "\n",
    "\n",
    "# Store the TF-IDF score in a csv file\n",
    "df = pd.DataFrame(Train_X_Tfidf.toarray())\n",
    "df.to_csv('./Features/tf_idf.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SMOTE to the training data to balance the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we want to apply SMOTE to the data to balance the data against 3 classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(Train_X_Tfidf, Train_Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  79.87601335240821\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "# What is the best kernel for SVM in the case of multi-class classification? - Quora\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(X_res,y_res)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for each class ->  [0.3853211  0.30674847 0.88156008]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the F1 score for each class\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score for each class -> \",f1_score(Test_Y, predictions_SVM, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
